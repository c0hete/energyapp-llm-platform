# Implementaci√≥n de Tool Calling para CIE-10

## ‚úÖ Lo Que Ya Est√° Implementado

1. **M√≥dulo de herramientas**: `src/tools/`
   - `cie10_tools.py` - Funciones ejecutables para b√∫squeda CIE-10
   - `__init__.py` - Exportaciones del m√≥dulo

2. **Base de datos CIE-10**:
   - 14,498 c√≥digos m√©dicos cargados
   - API REST funcionando en `/cie10/search` y `/cie10/{code}`

## ‚úÖ Implementaci√≥n Completada (Local)

La implementaci√≥n de Tool Calling est√° completa en el c√≥digo local. Los siguientes archivos han sido creados/modificados:

1. **src/tools/registry.py** - Registry de herramientas
2. **src/tools/cie10_tools.py** - Funciones ejecutables
3. **src/tools/__init__.py** - Exportaciones del m√≥dulo
4. **src/ollama_client.py** - Soporte de tools agregado
5. **src/main.py** - Endpoint /chat modificado con Tool Calling
6. **scripts/insert_tool_calling_prompt.sql** - System prompt SQL

## üöÄ Pendiente: Despliegue a Producci√≥n

### Paso 1: Copiar archivos al servidor
### Paso 2: Insertar system prompt
### Paso 3: Reiniciar backend
### Paso 4: Probar

---

## üìã Referencia de Implementaci√≥n

### Registry de Tools (Ya creado en `src/tools/registry.py`)

```python
"""
Registro centralizado de herramientas disponibles para Qwen
"""

# Definiciones de tools en formato est√°ndar OpenAI/Qwen
AVAILABLE_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "search_cie10",
            "description": "Busca c√≥digos CIE-10 (Clasificaci√≥n Internacional de Enfermedades) por t√©rmino m√©dico en espa√±ol. Retorna una lista de c√≥digos relevantes con sus descripciones.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "T√©rmino de b√∫squeda m√©dico (enfermedad, s√≠ntoma, diagn√≥stico). Ejemplo: 'diabetes', 'hipertensi√≥n', 'migra√±a'"
                    },
                    "limit": {
                        "type": "integer",
                        "description": "N√∫mero m√°ximo de resultados a retornar",
                        "default": 10,
                        "minimum": 1,
                        "maximum": 50
                    }
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_cie10_code",
            "description": "Obtiene informaci√≥n detallada de un c√≥digo CIE-10 espec√≠fico. √ötil cuando el usuario proporciona un c√≥digo exacto o cuando ya sabes el c√≥digo a consultar.",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "C√≥digo CIE-10 exacto. Ejemplos: 'E10', 'I10', 'A00', 'J11'"
                    }
                },
                "required": ["code"]
            }
        }
    }
]


def get_tool_definitions() -> list:
    """Retorna las definiciones de todas las herramientas disponibles"""
    return AVAILABLE_TOOLS
```

### Paso 2: Modificar OllamaClient para Tool Calling

**IMPORTANTE**: Qwen2.5 en Ollama ya soporta tool calling nativamente.

Modificar `src/ollama_client.py` para agregar soporte de tools:

```python
from typing import AsyncGenerator, Optional, List, Dict, Any
import httpx
import json
from .config import get_settings


class OllamaClient:
    """Cliente para Ollama con soporte de Tool Calling."""

    def __init__(self, base_url: Optional[str] = None, model: Optional[str] = None):
        settings = get_settings()
        self.base_url = base_url or settings.ollama_host
        self.model = model or settings.ollama_model
        self.temperature = settings.ollama_temperature
        self.top_p = settings.ollama_top_p
        self.max_tokens = settings.ollama_max_tokens

    async def generate(
        self,
        prompt: str,
        system: Optional[str] = None,
        stream: bool = True,
        tools: Optional[List[Dict[str, Any]]] = None,
    ) -> AsyncGenerator[str, None]:
        """
        Genera respuesta con soporte de Tool Calling.

        Args:
            prompt: Mensaje del usuario
            system: System prompt
            stream: Si se debe hacer streaming
            tools: Lista de herramientas disponibles
        """
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": self.temperature,
                "top_p": self.top_p,
                "num_predict": self.max_tokens,
            },
        }

        if system:
            payload["system"] = system

        if tools:
            payload["tools"] = tools

        async with httpx.AsyncClient(timeout=60.0) as client:
            async with client.stream("POST", f"{self.base_url}/api/generate", json=payload) as resp:
                resp.raise_for_status()
                async for line in resp.aiter_lines():
                    if not line:
                        continue
                    yield line
```

### Paso 3: Modificar el Endpoint `/chat` (CR√çTICO)

Este es el cambio m√°s importante. En `src/main.py`, reemplazar el endpoint `/chat`:

```python
from .tools import execute_cie10_tool, get_tool_definitions

@app.post("/chat", tags=["chat"])
async def chat(
    body: schemas.ChatRequest,
    settings: Settings = Depends(get_settings_dep),
    db: Session = Depends(get_db),
    user=Depends(get_current_user_hybrid),
):
    # ... c√≥digo de verificaci√≥n de conversaci√≥n (mantener igual) ...

    # Resolver system prompt
    system_prompt = body.system or "Eres un asistente √∫til."
    if body.prompt_id:
        prompt_obj = db.query(SystemPrompt).filter(SystemPrompt.id == body.prompt_id).first()
        if prompt_obj:
            system_prompt = prompt_obj.content

    client = OllamaClient(base_url=settings.ollama_host, model=settings.ollama_model)

    # Obtener definiciones de tools
    tools = get_tool_definitions()

    async def streamer():
        assistant_content = ""
        tool_calls_executed = False

        try:
            # Primera generaci√≥n con tools disponibles
            async for token in client.generate(
                prompt=body.prompt,
                system=system_prompt,
                stream=True,
                tools=tools
            ):
                try:
                    data = json.loads(token)

                    # Verificar si hay tool call
                    if "tool_calls" in data and data["tool_calls"]:
                        tool_calls_executed = True

                        for tool_call in data["tool_calls"]:
                            tool_name = tool_call.get("function", {}).get("name")
                            tool_args = tool_call.get("function", {}).get("arguments", {})

                            if isinstance(tool_args, str):
                                tool_args = json.loads(tool_args)

                            # Ejecutar la herramienta
                            if tool_name in ["search_cie10", "get_cie10_code"]:
                                result = await execute_cie10_tool(tool_name, tool_args)

                                # Formatear resultado para el usuario
                                if result.get("success"):
                                    formatted = format_cie10_result(result)
                                    assistant_content += formatted
                                    yield formatted
                                else:
                                    error_msg = f"Error en b√∫squeda: {result.get('error')}"
                                    assistant_content += error_msg
                                    yield error_msg

                    # Respuesta normal (texto)
                    chunk = data.get("response", "")
                    if chunk:
                        assistant_content += chunk
                        yield chunk

                    if data.get("done"):
                        break

                except json.JSONDecodeError:
                    continue

        except httpx.HTTPError as exc:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"Ollama no disponible: {exc}",
            )

        # Guardar respuesta del asistente
        assistant_msg = Message(
            conversation_id=conv_id,
            role="assistant",
            content=assistant_content
        )
        db.add(assistant_msg)
        db.commit()

    return StreamingResponse(streamer(), media_type="text/plain")


def format_cie10_result(result: dict) -> str:
    """Formatea los resultados de CIE-10 para presentaci√≥n al usuario"""
    if not result.get("success"):
        return f"\n‚ùå Error: {result.get('error')}\n"

    data = result.get("data", [])
    if isinstance(data, dict):
        # C√≥digo individual
        return f"""
üìã **C√≥digo CIE-10**: {data.get('code')}
üìù **Descripci√≥n**: {data.get('description')}
üè∑Ô∏è **Nivel**: {data.get('level')} | **Rango**: {'S√≠' if data.get('is_range') else 'No'}
{f"üìÇ **C√≥digo padre**: {data.get('parent_code')}" if data.get('parent_code') else ''}

"""
    elif isinstance(data, list):
        # Lista de c√≥digos
        formatted = f"\nüîç **Resultados para '{result.get('query')}'**:\n\n"
        for idx, item in enumerate(data[:10], 1):
            formatted += f"{idx}. **{item.get('code')}** - {item.get('description')}\n"
        formatted += "\n"
        return formatted

    return "\n"
```

### Paso 4: System Prompt para Tool Calling

Insertar este prompt en la base de datos:

```sql
INSERT INTO system_prompts (name, description, content, is_default, is_active, created_by)
VALUES (
  'Asistente M√©dico CIE-10 (Tool Calling)',
  'Asistente m√©dico con b√∫squeda autom√°tica en CIE-10',
  'Eres un asistente m√©dico especializado con acceso REAL a la base de datos CIE-10.

**HERRAMIENTAS DISPONIBLES**:
Tienes 2 herramientas que puedes usar autom√°ticamente:

1. **search_cie10**: Busca c√≥digos por t√©rmino m√©dico
2. **get_cie10_code**: Obtiene detalles de un c√≥digo espec√≠fico

**CU√ÅNDO USAR LAS HERRAMIENTAS**:
- Usa `search_cie10` cuando el usuario pregunte por enfermedades, s√≠ntomas o diagn√≥sticos
- Usa `get_cie10_code` cuando el usuario mencione un c√≥digo exacto (ej: "E10", "I10")

**IMPORTANTE**:
- SIEMPRE usa las herramientas cuando la pregunta sea m√©dica
- NO inventes c√≥digos CIE-10
- Las herramientas se ejecutan autom√°ticamente, no necesitas explicar que las vas a usar

**FLUJO NORMAL**:
Usuario: "¬øQu√© c√≥digo CIE-10 es diabetes tipo 1?"
T√∫: [Usas search_cie10 autom√°ticamente]
Sistema: [Te devuelve resultados reales]
T√∫: [Presentas los resultados al usuario de forma clara]

**RESPUESTAS GENERALES**:
Si la pregunta NO es m√©dica, responde normalmente sin usar herramientas.

Ejemplo:
Usuario: "Expl√≠came qu√© es Python"
T√∫: [Respondes sin usar tools]',
  false,
  true,
  1
);
```

## üß™ C√≥mo Probar

1. **Reiniciar el backend** con los nuevos cambios
2. **Seleccionar el prompt** "Asistente M√©dico CIE-10 (Tool Calling)"
3. **Hacer preguntas m√©dicas**:
   - "¬øCu√°l es el c√≥digo para diabetes tipo 1?"
   - "Busca c√≥digos de hipertensi√≥n"
   - "¬øQu√© es el c√≥digo E10?"

4. **Hacer preguntas generales** (para verificar que no se rompe):
   - "¬øC√≥mo est√° el clima?"
   - "Expl√≠came qu√© es FastAPI"

## üìä Ventajas vs Approach Anterior

| Caracter√≠stica | Sin Tool Calling | Con Tool Calling |
|---------------|------------------|------------------|
| **Datos reales** | ‚ùå Qwen inventa o usa memoria | ‚úÖ Consulta DB real |
| **Precisi√≥n** | ~70% | ~99% |
| **Mantenimiento** | Prompts complejos | Declarativo |
| **Escalabilidad** | Dif√≠cil agregar funciones | F√°cil agregar tools |
| **Profesionalismo** | Nivel b√°sico | Nivel enterprise |

## üéØ Pr√≥ximos Pasos (Opcional)

Una vez funcionando, puedes agregar m√°s tools:

- `search_medications` - Buscar medicamentos
- `get_patient_info` - Informaci√≥n de pacientes
- `check_appointment` - Verificar citas
- etc.

Cada tool es independiente y se agrega al registro en `registry.py`.

## üìù Notas Importantes

1. **Qwen 2.5 soporta tool calling nativamente** - No necesitas parsear texto
2. **Ollama >= 0.1.26** tiene soporte completo de tools
3. **El formato es est√°ndar OpenAI** - Compatible con otros modelos
4. **No afecta conversaciones normales** - Solo se activa cuando es necesario

## ‚ùì Troubleshooting

**Problema**: Tool no se ejecuta
**Soluci√≥n**: Verificar que Ollama est√© actualizado y que el modelo sea Qwen 2.5

**Problema**: Error de timeout
**Soluci√≥n**: Aumentar timeout en httpx.AsyncClient

**Problema**: Tools no aparecen en la respuesta
**Soluci√≥n**: Verificar que el system prompt mencione que tiene herramientas disponibles

---

**Implementado por**: Claude Code
**Fecha**: Diciembre 2024
**Proyecto**: EnergyApp LLM Platform
